DATASETS = ["cube", "afa_context"]
DATASET_INSTANCE_INDICES = [0, 1]
INITIALIZER = "cold"  # assume same initializer for now
UNMASKER_PER_DATASET = {"cube": "direct", "afa_context": "direct"}
EVAL_DATASET_SPLIT = "val"  # whether to use "val" or "test" during evaluation
DEVICE = "cpu"

# Main parameters that we investigate
# - hard budget during training
# - hard budget during evaluation
# - soft budget parameter during training
# - soft budget parameter during evaluation
PARAMS_PER_DATASET = {
    "cube": [
        (5, 5, "null", "null"),
        (10, 10, "null", "null"),
        ("null", "null", 0.4, "null"),
        ("null", "null", 0.6, "null"),
    ],
    "afa_context": [
        (10, 10, "null", "null"),
        (15, 15, "null", "null"),
        ("null", "null", 0.5, "null"),
        ("null", "null", 0.7, "null"),
    ],
}


rule all:
    input:
        "extra/plot_results/random_dummy",


rule train_method:
    input:
        "extra/data/datasets/{dataset}/{dataset_instance_idx}",
    output:
        directory(
            "extra/trained_methods/random_dummy/dataset-{dataset}+instance_idx-{dataset_instance_idx}/train_hard_budget-{train_hard_budget}+train_soft_budget_param-{train_soft_budget_param}"
        ),
    params:
        unmasker=lambda wildcards: UNMASKER_PER_DATASET[wildcards.dataset],
    shell:
        """
        python scripts/train/randomdummy.py \
            dataset_artifact_path={input} \
            save_path={output} \
            components/initializers@initializer={INITIALIZER} \
            components/unmaskers@unmasker={params.unmasker} \
            train_hard_budget={wildcards.train_hard_budget} \
            train_soft_budget_param={wildcards.train_soft_budget_param} \
            device={DEVICE} \
            seed=null \
            use_wandb=false
        """


# rule train_classifier:
#     input:
#         "extra/data/datasets/{dataset}/{dataset_instance_idx}",
#     output:
#         directory(
#             "extra/trained_classifiers/masked_mlp_classifier/dataset-{dataset}+instance_idx-{dataset_instance_idx}"
#         ),
#     shell:
#         """
#         python scripts/train/masked_mlp_classifier.py \
#             dataset_artifact_path={input} \
#         """


rule eval_method:
    input:
        "extra/data/datasets/{dataset}/{dataset_instance_idx}",
        "extra/trained_methods/random_dummy/dataset-{dataset}+instance_idx-{dataset_instance_idx}/train_hard_budget-{train_hard_budget}+train_soft_budget_param-{train_soft_budget_param}",
        # "extra/trained_classifiers/masked_mlp_classifier/dataset-{dataset}+instance-{dataset_instance}",
    output:
        "extra/eval_results/random_dummy/dataset-{dataset}+instance_idx-{dataset_instance_idx}/train_hard_budget-{train_hard_budget}+train_soft_budget_param-{train_soft_budget_param}/eval_hard_budget+{eval_hard_budget}+eval_soft_budget_param-{eval_soft_budget_param}/eval_data.csv",
    params:
        save_path=lambda wildcards: f"extra/eval_results/random_dummy/dataset-{wildcards.dataset}+instance_idx-{wildcards.dataset_instance_idx}/train_hard_budget-{wildcards.train_hard_budget}+train_soft_budget_param-{wildcards.train_soft_budget_param}/eval_hard_budget+{wildcards.eval_hard_budget}+eval_soft_budget_param-{wildcards.eval_soft_budget_param}",
        unmasker=lambda wildcards: UNMASKER_PER_DATASET[wildcards.dataset],
    shell:
        """
        python scripts/eval/eval_afa_method.py \
            method_artifact_path={input[1]} \
            components/initializers@initializer={INITIALIZER} \
            components/unmaskers@unmasker={params.unmasker} \
            dataset_artifact_path={input[0]} \
            save_path={params.save_path} \
            dataset_split={EVAL_DATASET_SPLIT} \
            classifier_artifact_path=null \
            seed=null \
            device={DEVICE} \
            eval_only_n_samples=null \
            batch_size=1 \
            hard_budget={wildcards.eval_hard_budget} \
            use_wandb=false
        """


rule merge_eval:
    input:
        [
            f"extra/eval_results/random_dummy/dataset-{dataset}+instance_idx-{dataset_instance_idx}/train_hard_budget-{train_hard_budget}+train_soft_budget_param-{train_soft_budget_param}/eval_hard_budget+{eval_hard_budget}+eval_soft_budget_param-{eval_soft_budget_param}/eval_data.csv"
            for dataset in DATASETS
            for dataset_instance_idx in DATASET_INSTANCE_INDICES
            for (
                train_hard_budget,
                eval_hard_budget,
                train_soft_budget_param,
                eval_soft_budget_param,
            ) in PARAMS_PER_DATASET[dataset]
        ],
    output:
        "extra/merged_eval_results/random_dummy.csv",
    shell:
        """
            csvstack {input} > {output}
        """


rule plot:
    input:
        "extra/merged_eval_results/random_dummy.csv",
    output:
        directory("extra/plot_results/random_dummy"),
    shell:
        """
            Rscript scripts/plotting/plot_eval.R {input} {output}
        """
